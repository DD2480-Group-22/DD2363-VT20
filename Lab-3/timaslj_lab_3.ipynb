{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timaslj-lab-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/timaslj/Lab-3/timaslj_lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2: Matrix factorization**\n",
        "**Timas Ljungdahl**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "source": [
        "In this report, 4 different algorithms were implemented and tested. The algorithms were sparse matrix-vector multiplication for matrices of CRS format, modified Gram-Schmidt iteration for QR-factorization, backwards substitution for solving $Rx = b$ and finally QR eigenvalue algorithm for finding the eigenvalues and eigenvectors for a matrix $A$. All algorithms were implemented and tested with random data and generally generated results with around 10 decimal precision. The results are probably highly affected by floating point error that occur when continuously adding and subtracting numbers of different magnitude. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo",
        "colab_type": "text"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "f74fa781-413b-41e7-a2ec-1bba2288ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import unittest\n",
        "import random\n",
        "import math\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "source": [
        "In this report, systems of linear equations are investigated of the form $Ax = b$\n",
        "where we want to solve for $x$ given a matrix $A$ and vector $b$. A system of linear equations has an exact solution if A is nonsingular, meaning that A has an inverse, since then $x = A^{-1}b$. In this report we assume that A is a square, nonsingular matrix. \n",
        "\n",
        "The direct solution methods implemented in this report are based on factorization of A into easily invertable matrices - diagonal, orthonormal and triangular matrices. The problem of solving for $x$ is summerized below.\n",
        "\n",
        "$Ax = b$, solve for $x$\n",
        "1. Factorize $A$ into $QR$, where $Q$ is an orthonormal matrix and $R$ is an upper triangular matrix. This gives us $QRx = b$.\n",
        "2. Since $Q$ is orthonormal $Q^{-1} = Q^T$. Orthonormal, means that each column vector in the matrix is normalized and orthogonal to each other. Multiplying $Q^{-1}$ on the left on each side, we get $Q^{-1}QRx = Q^{-1}b => Rx = Q^Tb$.\n",
        "3. We can now easily solve $Rx = Q^Tb$ \n",
        "\n",
        "Apart from solving systems of equations, sparse matrix-vector multiplication was implemented for sparse matrices of compressed row storage(CRS)format. Instead of storing all values in the matrix, only the nonzero values are stored in an array ***v***, along with the two index arrays ***col_idx*** and ***row_ptr***. There is an entry in the ***col_idx*** array for each value to indicate the column of the value in the original matrix. The ***row_ptr*** array consists of the index in ***v*** where each row starts. \n",
        "\n",
        "The QR eigenvalue algorithm was also implementet which for a real symmetric matrix $A$, returns a unitary matrix $U$ with the eigenvectors of $A$ as column vectors and a upper triangular matrix $T$ with eigenvalues of $A$ in the diagonal. It finds the a Schur factorization such that $A = UTU^{*}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "source": [
        "# **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "source": [
        "To achieve the goal of solving the system of equations, the first step is to implement an effecient factorization method. In this report, a modified version of Gram-Schmidt iteration is implemented. This method recursively computes an orthonormal matrix $Q$ from $A$ by taking each column vector in $A$ and subtracting its projection onto the already computed orthonormal space in order to compute a new perpendicular vector which is then added to the set of orthonormal vectors. $R$ is then computed as $R = Q^TA$.  \n",
        "\n",
        "When $Q$ and $R$ have been computed, $b$ is multiplied by $Q^T$ on the left to form a new vector $b_{prim}$ so that $Rx = b_{prim}$. This new system of equations can now be solved with backwards subsitution as $R$ is upper triangular. This is possible since $x_n = b_n/a_{nn}$ which can then be substituted in order to solve for $x_{n-1} = (b_{n-1} - a_{(n-1)(n)}x_n)/a_{(n-1)(n-1)}$. This can be written as:\n",
        "\n",
        "$x_i = (b_i - \\sum_{j=i+1}^{n}a_{ij}x_j)/a_{ii} $\n",
        "\n",
        "In order to implement these algorithms, pseudocode from the lecture notes provided in class were followed. \n",
        "\n",
        "To test the QR factorization method,random $A \\epsilon \\mathbb{R}^{n \\times n}$ were generated with imported numpy methods. The output $Q$ and $R$ of the algorithm were then multiplied together again and asserted to be equal to $A$.\n",
        "\n",
        "To test the direct solver method, random $A \\epsilon \\mathbb{R}^{n \\times n}$ and $b \\epsilon \\mathbb{R}^{n}$ were generated and the residuals $|| Ax-b ||$ and $|| x-y ||$ were asserted to be equal to zero. \n",
        "\n",
        "To test the QR eigenvalue algorithm, a random real symmetric matrix A was generated and $det(A - \\lambda_iI)$ was asserted to be equal to 0. $||Av_i - \\lambda_iv_i||$ was also asserted to be equal to 0. The tests follow the definition of eigenvectors and eigenvalues: $Av = \\lambda v$ where $v$ is an eigenvector and $\\lambda$ is an eigenvalue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS2U_WWT15_4",
        "colab_type": "code",
        "outputId": "7ab8e578-910d-4b3c-c737-242a110f98b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "##can be done without saving R,D and D_inverse -> much more space efficient\n",
        "def jacobi_iteration(A,b, TOL = 1e-8):\n",
        "  n = A.shape[0]  \n",
        "  D_inverse = np.zeros((n,n))\n",
        "  for i in range(n):\n",
        "    D_inverse[i,i] = 1/A[i,i]\n",
        "\n",
        "  M = np.identity(n) - np.matmul(D_inverse, A)\n",
        "\n",
        "  c = np.matmul(D_inverse,b)\n",
        "\n",
        "  x = np.random.rand(n)\n",
        "\n",
        "  while np.linalg.norm(np.matmul(A,x)-b) > TOL:\n",
        "    x = np.matmul(M, x) + c\n",
        "\n",
        "  return x\n",
        "\n",
        "def gauss_seidel_iteration(A, b, TOL = 1e-8):\n",
        "  n = A.shape[0]  \n",
        "  L_inverse = A.copy()\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if (j > i):\n",
        "        L_inverse[i,j] = 0\n",
        "\n",
        "  L_inverse = np.linalg.inv(L_inverse)\n",
        "\n",
        "  M = np.identity(n) - np.matmul(L_inverse, A)\n",
        "\n",
        "  c = np.matmul(L_inverse, b)\n",
        "\n",
        "  x = np.random.rand(n)\n",
        "\n",
        "  while np.linalg.norm(np.matmul(A,x)-b) > TOL:\n",
        "    x = np.matmul(M, x) + c\n",
        "\n",
        "  return x\n",
        "    \n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  #left preconditioning so system converges\n",
        "  def test_random_sparse_matrix(self):\n",
        "    size = random.randint(2, 50) \n",
        "    A = np.random.rand(size, size)\n",
        "    alpha = 1.1\n",
        "    B = np.linalg.inv(A)*alpha #approx inverse of A so ||I-alpha*B*A|| < 1\n",
        "    C = np.matmul(A,B)\n",
        "    x = np.random.rand(size)\n",
        "    b = np.matmul(C,x)\n",
        "\n",
        "    x_approx = gauss_seidel_iteration(C, b)\n",
        "    print(x_approx)\n",
        "    print(x)\n",
        "\n",
        "    #print(x)\n",
        "    #x_prim = jacobi_iteration(A,b)\n",
        "    \n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "."
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.59375668 0.41458276 0.7088139  0.03278796 0.43836699 0.12414629\n",
            " 0.23843034 0.97403904 0.8738789  0.61685779 0.44859183 0.64692526\n",
            " 0.04297032 0.32615792 0.55055631 0.98184408 0.44004754 0.82152972\n",
            " 0.99683997 0.13093163 0.78075986 0.98283926 0.84871536 0.47386535\n",
            " 0.53184105 0.80384882 0.76745939 0.56019984 0.07798569 0.40709451\n",
            " 0.57322915 0.27859381 0.04679653 0.92919737 0.91002934 0.20163357]\n",
            "[0.59375668 0.41458276 0.7088139  0.03278796 0.43836699 0.12414629\n",
            " 0.23843034 0.97403904 0.8738789  0.61685779 0.44859183 0.64692526\n",
            " 0.04297032 0.32615792 0.55055631 0.98184408 0.44004754 0.82152972\n",
            " 0.99683997 0.13093163 0.78075986 0.98283926 0.84871536 0.47386535\n",
            " 0.53184105 0.80384882 0.76745939 0.56019984 0.07798569 0.40709451\n",
            " 0.57322915 0.27859381 0.04679653 0.92919737 0.91002934 0.20163357]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.012s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMcBhgAGVAIb",
        "colab_type": "code",
        "outputId": "dd60b013-fe00-45df-8c7e-15a57ec418fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "def get_random_func(n):\n",
        "  exponents = []\n",
        "  #func_string = ''\n",
        "  scalar = random.randint(0,1000)\n",
        "  for i in range(n):\n",
        "    exp = random.randint(1,5)\n",
        "    exponents.append(exp)\n",
        "    #func_string += 'x' + str(i) + '^' + str(exp) + ' + ' \n",
        "  #print(func_string + str(scalar))\n",
        "\n",
        "  def random_func(x):\n",
        "    sum = 0\n",
        "    for i in range(n):\n",
        "      sum += x[i]**exponents[i]\n",
        "    return sum + scalar\n",
        "  return random_func\n",
        "\n",
        "def scalar_jacobian(f, x, dx=1e-8):\n",
        "  fx = f(x)\n",
        "  J = np.zeros(x.shape[0])\n",
        "  dxi = x.copy()\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    dxi[i] = dxi[i] + dx\n",
        "    J[i] = abs(f(dxi)-fx)/dx \n",
        "    dxi[i] = x[i]\n",
        "  \n",
        "  return J\n",
        "\n",
        "def scalar_newtons_method(f, x0, TOL = 1e-8):\n",
        "  x = x0\n",
        "\n",
        "  while abs(f(x)) > TOL:\n",
        "    df = jacobian(f, x)\n",
        "    for i in range(x.shape[0]): \n",
        "      if not(math.isclose(df[i],0)): #divide by zero\n",
        "        x[i] -= f(x)/df[i]\n",
        "      else:\n",
        "        return None \n",
        "      \n",
        "  return x\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "\n",
        "  #left preconditioning so system converges\n",
        "  def test_random_functions(self):\n",
        "    for n in range(1000):\n",
        "      size = random.randint(2,10)\n",
        "      f = get_random_func(size)\n",
        "      x0 = np.zeros(size, dtype='float64')\n",
        "\n",
        "      root_approx = scalar_newtons_method(f,x0)\n",
        "\n",
        "      if root_approx is None:\n",
        "        continue\n",
        "\n",
        "      f_root = f(root_approx)\n",
        "    \n",
        "      if f_root == f_root: #not NaN\n",
        "        self.assertAlmostEqual(f_root, 0, 5)\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 969,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.101s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQE6adceByNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "744730fb-edc0-4b8c-aca6-15997ed44235"
      },
      "source": [
        "def vector_jacobian(f, x, dx=1e-6):\n",
        "  fx = f(x)\n",
        "  number_of_variables = fx[1]\n",
        "  J = np.zeros((fx[0].shape[0], number_of_variables), dtype='float64')\n",
        "  dxi = x.copy()\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    dxi[i] = dxi[i] + dx\n",
        "    fdxi = f(dxi)\n",
        "    J[:,i] = (fdxi[0]-fx[0])/dx \n",
        "    dxi[i] = x[i]\n",
        "  \n",
        "  return J\n",
        "\n",
        "def vector_newtons_method(f, x0, TOL = 1e-8):\n",
        "  x = x0\n",
        "\n",
        "  fx = f(x)\n",
        "\n",
        "  nums_of_its = 0 #ta bort\n",
        "\n",
        "  while np.linalg.norm(fx[0]) > TOL:\n",
        "    fx = f(x)\n",
        "    df = vector_jacobian(f,x)\n",
        "    dx = np.linalg.solve(df, -fx[0])\n",
        "    x = x + dx\n",
        "    nums_of_its += 1\n",
        "\n",
        "  print(\"Nums of its: \", nums_of_its)\n",
        "  return x\n",
        "\n",
        "def function1(x):\n",
        "  return (np.array([(x[0]**2)*x[1], 5*x[0]+math.sin(x[1])], dtype='float64'),2)\n",
        "\n",
        "def function2(x):\n",
        "  return (np.array([x[0]*math.cos(x[1]), x[0]*math.sin(x[1])], dtype='float64'),2)\n",
        "\n",
        "def function3(x):\n",
        "  return (np.array([5*x[1], 4*(x[0]**2)*2*math.sin(x[1]*x[2]), x[1]*x[2]], dtype='float64'),3)\n",
        "\n",
        "def get_random_vector_func():\n",
        "  number_of_vars = random.randint(2,10)\n",
        "  exponents = []\n",
        "  scalars = []\n",
        "  for funcs in range(number_of_vars):\n",
        "    func_string = ''\n",
        "    scalars.append(random.randint(0,100))\n",
        "    func_exponents = []\n",
        "    for i in range(number_of_vars):\n",
        "      func_exponents.append(random.randint(1,10))\n",
        "      func_string += 'x' + str(i) + '^' + str(func_exponents[i]) + ' + ' \n",
        "    print(func_string + str(scalars[funcs]))\n",
        "\n",
        "    exponents.append(func_exponents)\n",
        "  \n",
        "  def random_vector_func(x):\n",
        "    fx = np.zeros(number_of_vars)\n",
        "\n",
        "    for funcs in range(number_of_vars):\n",
        "      sum = 0\n",
        "      for i in range(number_of_vars):\n",
        "        sum += x[i]**func_exponents[funcs][i]\n",
        "      fx[funcs] = sum + scalar\n",
        "  \n",
        "  return (random_vector_func, number_of_vars)\n",
        "\n",
        "\n",
        "#print(root_approx)\n",
        "root_approx=vector_newtons_method(function1, np.array([50,100000], dtype='float64'))\n",
        "print(root_approx)\n",
        "print(function1(root_approx))\n",
        "#fx = get_random_vector_func()\n",
        "#x0 = np.zeros(fx[1], dtype='float64')\n",
        "#root_approx = vector_newtons_method(fx, x0)\n",
        "#print(function3(root_approx))\n",
        "\n",
        "\n"
      ],
      "execution_count": 1080,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nums of its:  31\n",
            "[2.48871078e-07 1.00172823e+05]\n",
            "(array([ 6.20438549e-09, -1.35111465e-12]), 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La9RmlT9QXKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH8Ojla-4sm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(jacobian(np.array([[2,5],[8,-1]]), np.array([1,2])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "source": [
        "All algorithms were implemented and tested with random data several times. The precision of the algorithms differs but generally the output had a precision of around 10 decimals. The precision is probabaly affected by floating point errors that occur when adding and subtracting numbers of different magnitude. The modified Graham-Schmidt iteration, however, mitigates the absorption effect of floating point addition as the orthonormal set is generated recursively and does not rely on the summation of a large set of numbers.    "
      ]
    }
  ]
}